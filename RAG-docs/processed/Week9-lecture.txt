
okay all right so let me pop this up here
bigger i can do that there we go all right so we've been talking about decision trees today all right we'll build a
little bit maybe towards ensemble models towards the end but principally we're just going to talk about decision trees
we're going to talk about how they're made or give you some theoretical background related to uh rates of how they are how constructed
and how they're used they can be used for both regression and classification we're principally going to talk about classification
at this point which is the primary use case for them and then um i'll talk about some of the weaknesses and strengths generally all right so
let's just jump right into it here all right okay so this is what a little
ahead of myself trying to shrink my there we go okay okay
so uh i said all of that already so all right so let's talk right away
about uh about the way that trees work okay so when we think about them they're kind of this you know this upside down triangle
basically they kind of build out this way and they're constructed largely of these uh you know basically these
four things right so we have a root node which is basically this top note up here in a decision tree they have these internal
nodes which are basically these split nodes they come out like this all right so it has one edge
and these edges are just basically the lines that are connecting to connecting the nodes okay and then they
have a leaf node or terminal node kind of the same thing it has one node coming in one edge and then no outgoing edges
basically you think about them as kind of at the bottom of the tree okay so this is our this is our terminal
i'm sorry our root up here this is an internal node and then these are the leaf nodes and
then all of these things together have these edges that connect them all right hey it's really easy just to look at it this
way and so here what we see this is our this is our root note up here all right this is the old classic iris
iris data set that everybody just loves seeing over and over again so here we have the root nodes these are
internals and then down here on the bottom we see the leaves all right so this is a
matplotlib graphic actually coming out of python it
does a bit better job and we'll talk about kind of this all the components of this as we move along so what's going on
inside the boxes i don't have to totally understand but uh we'll take a look but this is
also important note here is that right so a leaf node doesn't have to be all the way down on the bottom all right then we have one right here
because we have no edges coming out of this basically what this is saying is that
petal width just to give you some context here pedal width does such a good job
of classifying setosa right that's remember that's a type of flower
that we have there does such a good job of classifying setosa there's no reason to move on as you can see here basically any any
petal width that is less than or equal to 1.75
all right is going to be fall into the subtosa class right essentially all right so let's
keep moving all right so that's that's it at a very high level is what we think about when
we we think about decision trees and again it's just like this a cyclical graph that kind of moves down all right but
how do they make these choices so let's let's talk about that all right so the most important question so what
is the most important question uh to move on to a second date right it seems to me
like uh you know are you married is probably a more important criteria than uh hey
what's your favorite music you know if you don't know this one i guess it depends on your your
your ethical standing but yeah in general you have to say to yourself what are these important questions that
that you need to ask or need to be informed of in order to move on to a second date all right so
when we think about that the question that the most amount of relevant information this is exactly how a decision tree works all
right they're going to need they want to be able to figure out which variable gives it the most relevant information
to be able to decide uh how to classify our target variable all right all right so
you know depending on the condition of the first answer all right then you might select you know the next most important
question for in terms of just information gain as maybe the question we're trying to work out here obviously is
you know whether we're going to move to a move to a second date all right so are you married all right if the answer
to this question is yes hopefully most people would say whoa okay well check please
all right so that that might be it all right if the answer here is yes is yes all right again you would stop
all right but then you know the question would be like what is the next series of questions that you might ask
all right you know belief in a blue colored sky you know maybe that's good because you could you know
get a sense of whether someone is insane or not or you know maybe then you do move on to say you know do we have more things in common you
know what are your favorite foods or music or whatever all right so if you if the answer was to
no then maybe you move on to the next variable which talks about something that is just kind of more personal to see if
you're you know if your likes and dislikes overlap okay
any question is like when should you stop asking questions all right so say you made it you passed the uh past the marriage
hurdle and now you're on to what music do you like right and so
here you know it might be that you know a certain person you know fifty percent will go on uh on
a date if they believe that they like you know acoustic and then maybe this this is disco or something and so you know 50
percent of people will go that so then you start getting very specific about likes and dislikes in terms of what type
of music alright so you can imagine this would be just a factor level variable that could have all sorts of different types of
classical and brock and you know the rap and
lots of other types of music which apparently i'm having trouble naming but anyway so you think about that and
depending on the answer to that maybe people would have more information about whether their likes or dislikes overlapped right okay all right
so this is basically how this how this works okay so uh it's fairly intuitive that's a big
reason that people like decision trees because of this nature you can kind of explain it in simple terms about how they're used and
how they make choices all right so we asked the question you know what is
what is the variable that has the most important information in terms of the ones and zeros that we're trying to
classify and here we're trying to classify whether someone is likely to go on a second date or not all right so we
asked this first question about marriage all right and that results similar to what we saw in the flower
example to maybe a perfect classification of whether they will go on to the second date or not right the majority of people would not
go on to a second date right knowing that someone was married but if the answer to this is no all right this would be no this would be
yes and then up above here we basically would be you know
second date is the variable we're trying to classify right so that would just be a list of
zeros and ones right zero being no second date and one being yes
okay excuse me and then we get more specific right based off the type of
music that people are interested in there might be more of a disjointed result
right okay so we asked the most important information and then conditioned on the first answer
we select the next most important question and when the answer is no longer providing any additional information we
stop growing the branch all right it looks like maybe this is a 50 50 split you know 50 50. something like that
we kind of stopped growing the blanche all right and we just do this all right and we'll talk about stopping criteria it also has a lot to do with
the complexity perimeter things like that or hyper parameters that you can tune to keep it above a certain level
all sorts of different things you just basically keep repeating these two steps over and over again until no more
information can be gained with the variables that you have all right so you use all the variables that
you have inside your inside your data set to try and gradually get to know better and better
whether someone will go on a second date or not all right okay all right
so decision trees are hierarchical all right what that means is basically um
they're built just in the way that you know we've kind of been viewing them is that they you know they start start small and grow
more complex all right decisions decisions are made until a predetermined metric is met and we will
talk about what that metric might look like all right it could be you know a certain amount of
terminal nodes all right predefined there's lots of different hyper parameters you could actually set a lot of people set the
number of layers say if you have you know a huge amount of data at your disposal
you know the more layers that are in a tree the more complex the decisions get all right all right and so
the model is is built such that a sequence of ordered decisions concerning the value result and you know at the end
the model inside the terminal nodes are the leaf nodes all right actually predicts the class label okay i'll give
you a percentage likelihood of that as well so it does it has a probabilistic output similar to k n it's non-parametric
uh it just means that you know the number of parameters is not determined all right as is the case with linear
models you know you kind of know what the variables are going to be with this it's it's not always
predestined right you could have a couple of variables that are incredibly good at predicting these classes you know marriage might be one
of them and so you actually wouldn't need that many to have a very high level of certainty or to hit some level of
metric that you're targeting or it could be that there are you have a bunch of weak variables so you need to have a very complex tree to
understand how people's decisions are are being made right and so we also don't need to worry about
any assumptions being met about distributions or perimeters okay
so there's no normality assumptions no standardization all right it's just a cyclical graph all
right so it's used in model probabilities and causality you can use there are causal trees a bit more
complicated to translate but trees consist of these nodes and edges and they're defined by this decision rule
and we're going to we're going to talk about what that rule is all right okay let's talk about the background a
little bit all right so it uses recursive binary
splitting which basically means that it it considers every possible partition space
and then it divides the one based off uh it chooses the partition space basically chooses where to split
a variable to locally optimize that information gain right so it does
not consider the future of the tree doesn't consider the past it only tries to optimize that
individual node those individual internal leaves okay all right
and it will consider every possible partition uh available when doing that okay so it
this is this is what we call kind of a greedy approach right it's kind of like localized optimality um optimality
optimization inside that one internal node okay so it's going to try and make the best choice it can
uh inside that internal node right so it's each step of the tree building
process it picks the best the best split for that particular that particular split it does not consider
the future all right all right so trees you know i talked about this they can be they can use be used for um prediction
or classification but they use recursive binary splitting in both cases all right um the difference between
regressive trees uh is that they're they're going to predict okay so classification trees are going
to predict the actual class whereas regression is going to predict a particular number all right
right okay so the probability measure um that derives this building for
classification comes in two forms all right so we have these these two um measures which are
generally used for are very typical it's either it's either a genie index or
enthalpy genie index is actually a bit more common you saw that on the on the graph that we showed earlier with the iris data set it actually gave you
the genie index at each internal node but we're going to go through both of them they're very similar they're both
related to basically to information gain which is the key component to kind of
understand how decision trees work is this idea about that they're they're selecting the variable and how to split that
up variable based off the idea that they are maximizing information gained locally at that particular node okay
all right all right so the background here this is
the cart algorithm all right um it's a classification and regression
tree all right it was first uh introduced here by 1984 with these four
researchers all right it can be used in numerical categorical data all right so the first or splits the
training data into two subsets all right i guess we kind of know that a single feature all right it searches through all the possible features
all right to identify the split that produces the purest subsets right so the the purest idea is that purity is
related to maximizing information gain okay all right so it stops once you can't find a pla a split all right that
that reduces impurity right or by some other pre-predetermined hyper parameter
all right so let me kind of explain a bit more as we go all right it
all right so that's a bit of a theoretical background of those two papers it really just takes kind of this idea of information gain and puts it into
practice so it's similar to those four steps that we talked about the card algorithm is basically those four steps
in practice you know we're repeating steps two and three all right okay it does have uh some pretty
some pretty weighty advantages right it is simple to understand and interpret through data visualization
this is a single tree remember you just produce a single tree all right it doesn't require almost any data preparation at all all right the
other techniques often require normalization or dummy variables need to be created and blank values need
to be removed you know trees have a lot of advantages that they do not have to do a lot of that all right so they're able to handle
numerical categorical data all right it uses it what we call a white box bottle all right so you can actually see what's
going on all right you can understand why it made the decisions it's made and uh it's very interpretable okay as
compared to maybe more black box models some neural network approaches even though we're getting better at explainable ai things like that um
and then even when you get up to ensemble models with random force they also become a little a little hard to
understand it's pretty straight straightforward to evaluate i mean most of the models nowadays are not too bad you can use
these the evaluation metrics that we went over um you know in you know last
week in the week before adding you know edit at a pretty high rate uh pretty much for all of these but
they're also very usable here for tree based methods um okay okay it does have some
limitations as all algorithms do so this is important to remember what they are otherwise uh
we'll be using to use uh use these methods in the wrong circumstances all right they do have
like a legitimate uh tendency to overfit you know if you're just growing a tree
without any type of restrictions you're almost guaranteeing that it's going to overfit linear models are the same way
so we have to control for those a bit all right so you have to think about you know if you have 100 you know if you hunt as an example if
you have 100 data points right you have six levels you're gonna have 64 terminal nodes right if you only have 100 data points
so you basically you don't have a lot of single a lot of single terminal nodes which means you're basically just memorizing
the data they're not going to generalize well and this is again this is our bias variance trade-off we'd want to use hyperparameter two need to
pull that back up or maybe some other uh metrics that we'll talk about in particular this this this idea of this complexity
parameter to set that to a certain level that minimizes the error associated with the predictions and the actual values
and that would be the metric that we try and target so there's lots of ways to be able to control this so it has this problem
we also can control it all right they can also be generally unstable to
small variations in the data because of the nature of how they're how they're designed all right so
this is why you know a single decision tree depending on how good or bad your data is can be very
useful but the truth of the matter is that often people build towards these ensemble models which like
range random forest which is just you know thousands hundreds or thousands of trees
to be able to compensate for kind of this instability associated with a single tree
all right all right so practical distribution tree algorithms are based off of you know
heuristic which is a greedy we talked about this they make local optimal decisions all right so they can't
guarantee that you'll get like the global global optimum all right similar to uh some
other methods like this is that it could be possible that you know you don't get the perfect tree which again is a reason to move
potentially towards ensembles all right all right so the great uh bias trees if
some classes dominate it is there for recommended that you try and balance the data set prior to
fitting all right so if you have a heavily unbalanced data set and this is true of many of the classifiers we're going to be talking about it just doesn't perform
quite as well all right support vector machines actually have a tendency to work a little bit better as an example for
unbalanced data sets a single tree can can be can be can be problematic with data sets that are
quite unbalanced but again that can be cured by moving to kind of more of an ensemble approach
but until we get there we have to understand what one tree is doing before we start building forests all right so let's let's take a look all
right these mathematical approaches and i have some examples all right okay so decision trees uh use several
types of node splitting criteria all right so these are these are those i've talked about these
uh this is what it would be if we had you know kind of a continuous data problem we're trying to actually predict the value all right as
compared to predict the class and these are the two common ones for classification they're really very
similar all right we're going to take a look at we're actually not going to take a look at this one but we're going to take a look at each one of these
and they both use this idea of information gain to determine variable split criteria all
right so let's what is this crazy information gain thing all right so this is enthropy
all right all right so that's the that's the um
that's the equation there all right so basically it's just this all right so it's the the probability
times the log probability of a particular instance you're going to summarize these
together all right so just let me let me walk you through this instead of detailing through the equation here
all right so what we would see here okay so this is we have a total set of six all
right so we're thinking about basically this is our equation in
practice here just this portion of it so we're going to sum these two up and then we're going to subtract them from each other and then that's going to give us base
the general enthalpy of these six dots
all right and you can see here what we have is basically perfect classification all right so or maybe
where we have uh three and three all right so when we subtract these two things from each other
right we just get kind of the net entropy associated with this example right we do the same thing here
we have a little bit of more discontinuity right so we have four and two okay so we just plug and
chug basically into our equation we're just doing the probability here which is two out of six
all right and then probability here so four out of six we're just multiplying that by the
log two of exactly that same ratio and then subtracting them from each other just to get an idea of the balance right
here so what we see here right it's just a total you know kind of one class example obviously we we don't
have to do the blue calculation because we know it's zero and so we just do the green side of it which is going to result in basically no
information so you can so we can put these let's extend this a bit further and just kind of
clumsy through that all right the idea here is that information gain is basically going to be right the
entropy of the parent right which is what we did up here this is the enter fee think of this as the calculations of the
parent all right this is the by the parent i mean before the split all right so this is just what our data looks like before we
before we use any variable examples to be able to split it right
and then it's going to be the minus the average entropy of the children right and the children
are going to be what it looks like you know after some type of decision criteria
has been made okay so you can think of this as up here what we have this is entropy the
parent so we have kind of a perfect division here which is which is which is good maybe that's what we want um
you could think of this as you know pluses maybe went on the date right so that's a it
went on a second date just to continue to extend this example
and the dots all right did not
okay and then we're gonna then we apply some type of question right so this might be i don't know this
might be the marriage question right so
all marriage just pretend i know how to spell all right and so and this would be maybe this is yes
sorry this is a yes category some people are married all right so as a result um
what we see here is that if the answer is yes all right that two of these uh
two of these dots stop right there all right so that's an entropy it's a perfect classification relative to all right no date all right so see no
date which makes sense all right so they yes all right the people the no date uh stop right there all right but but one
person you know this is uh so no all right so despite that
there is some uh all right blending of that and it could just be that you know
whatever that might be is that there's there's no date here all right so what we see is
that there are four out of the all right original six continue on
and probably what we see here in this green dot is maybe there's some other reason why that second date didn't occur all right so we'd have to continue
making splits but for this there's no there's no additional reason for us to continue here right because we have
basically a terminal node okay so we've classified this perfectly
associated with this one no date class and that's exactly what we talked about kind of in the beginning
about how good a particular question would be but here all right we have some we have
some blending right so we don't see a zero so we wouldn't stop we'd continue to go okay all right
now in order to be able to calculate this and just erase some of this trying to put this in context all right
we're gonna take the average of these two numbers right so the average of 0.8
0.0 all right and we're going to subtract that from the parent and that will tell us the quality
essentially of this marriage question or it would give us the net effect in terms of information gain for this
particular question at this particular partition all right so this is a is a yes no so
there's just the partition is simple it's just did they get did are they married or not but if it's a continuous variable
basically try uh every type of potential partition inside that range of
values calculate the information gain to see where it makes the most sense which is the example we saw for um
i guess it was pedal width right so at a certain numerical range 1.75
or greater or was it less anyway at a certain threshold associated with that that distance for that pedal they
had perfect classification for that flower so that's where that um split criteria occurred okay so it
shows all the different potential ranges inside that basically all the the the potential
partitions inside that continuous variable that are represented in the data frame all right they're represented in that vector and chose
that one as being the best because it provided almost perfect information for that particular flower all right so what we would do here this
is a now it's a weighted average because it takes into consideration what the original
split was right okay and so we just subtract uh we add these two things together all
right and then we just subtract it from one and so we get 0.54
all right okay so that would basically be our our net gain there associated with um
associated with our uh information information gain all right so what we would do to finish
this is what we see here is this .54 is the result of this weighted average remember that this is
our calculation of our enter fee previously we just add these together
0.54 then we would subtract that from one and then here we would say we have a general information gain
of 0.46 okay all right so now keep in mind if it was
perfect all right and this resulted in a perfect split as in the extreme examples which sometimes we'd like to refer to that
this would basically just be zero right and then we'd have you know one minus zero we'd have a perfect information gain
of one sorry about that all right and so we would we wouldn't have to move
on any further we'd have a variable that perfectly classifies into date and no date as an example
right but we don't have that all right it's not a perfect classifier it's somewhere in between it's perfect for this one side but there's still lots of information
that could be gained uh and specific to to the um this is the you know when on a date
category so we need to continue to continue to grow this tree out so we could better understand that particular
side of the uh side of the equation okay so let's take a look at another
another example here all right so what we see in order to start the tree right we need to follow again just these
these basically these three steps right i keep giving you steps here but all right we choose a attribute with the
highest information gain okay so we can check child nodes and then we repeat repeat steps one and two you know we're
cursively into uh no more information can be gained all right
so let's take a look at that all right so this is a pretty common example that gets used but what
we're trying to do here is to predict you know should we play outside i think the original example here is like should
we play tennis outside i think are we going to be able to play tennis something like that all right so we have these three variables right this is our
predictor right we're trying to predict um whether play happened outside or not and we have
these variables to help us like the outlook you know the temperature and then the you know the humidity all in
classification all right all in um factors right so let's take a look so this is our our
original variable all right so when we look here yes and no all right that's our play so we have a perfect
all right we've got a perfect information perfect entropy here here we had two that went and played and
two that did not so that that's our parent node at one all right and then we're taking a look at this first
variable here sunny so that resulted in one yes all right all right
and uh two nodes two nodes all right so we have three sunny variables all
right and then when we look across what we see is we have two nodes and one yes all right so it doesn't give
us perfect information all right but it does give us some pretty good information that when it is sunny
we have one yes and two nodes all right but then we have you know when it's cloudy which is also
here what we see so we have one yes all right so we're trying to figure out
play or not play which is at the top and the first variable that we took a look at all right we haven't determined
if this is the best variable for us yet but the first one that we took a look at is outlook so we just did this
combination again we see a zero here we see a 0.92 here and then we're just
going to do this weighted average and then subtract it from our original
original entropy of our target variable okay so this is the weighted average
we're just going to do this 3x4 because remember we had three
three sunnies and one cloudy all right i'm just going to multiply that by the by our enthalpy gain here which is just
this formula that we talked about earlier and we're going to add that to the other one which is going to be
going to be 0 right okay and then we're going to subtract it by 1. so we're going to get all right 0.31
okay so let's keep going let's evaluate let's do this same process and we'll look at some of the other variables
all right so let's look at hot and cold all right so what we see here all right
we see temperature if the temperature is hot right no play no play the temperature is cool
it did play and they did play all right we're giving it away here but this is going to result in perfect classification all right so what we're
going to see here all right is that the relative to the entropy uh that was seen in the target variable
when we do this weighted average obvious this is all going to be zero just like i said before it's going to result in a perfect
perfect information gain okay so here this perfectly matches our
outlier or perfectly matches our target variable whereas the example we had before didn't it
wasn't quite a perfect split right we had one yes over here and we have one yes over here so when we calculate
information gain it's not quite as good as when we do it here right we have perfect information gain
all right so let's we're talking a we've seen an in-between example we've seen a perfect example so now let's look at a
perfectly terrible example all right so here what we see is we have perfect disagreement okay
regardless of whether it's high or low it looks like there was a decision to play and to not play all right so here we
have one yes and one no on both sides of this high
low equation so what we're going to see is when we plug this into our uh you know our enthalpy
equation and we subtract these from each other uh we're going to get one all right which which is basically means
that you know we're not we don't learn anything all right the result of this is one restrict one from one
our net information gain here is going to be absolutely nothing okay so if we were to build this
classifier uh what we would see here this is this is very uncommon of course
but what we see is that our we have this temperature oops
temperature would be right at the top all right so that would be the first variable that we would use it provides
us the most information and then the next one would be outlook i mean actually technically we wouldn't even move on to a next one because we
have perfect classification here so we wouldn't even need another variable so in this in this case when you have perfect information gain
um you just have a you know a tree with the depth of just one split and that's it okay
so that's enthropy all right so information gain is is very similar all right all right so it basically uses
the same idea about net gain all right so pi in this equation just represents
the probability that a random selection would have state i whether it would have you know you think it was whether
someone would be would play or not all right and so the mathematical process this
is pretty much the same i don't think i put the actual i thought i had the actual equation in here but
uh anyway you kind of get the idea here so it's this is just the same all right this is the example that we saw before
all right relative to what the outlook was like right so this
is the outlook calculation it's this one right here okay so we use the same
same general idea i'm just going to show it using the genie coefficient
the difference is that there's not it does not actually include this log conversion okay
which uh is is most of the reason that uh that most of the packages and uh
techniques associated with uh information gain and this uh criteria for selection have a
tendency to default to gini to genie index because it's just not as computationally oh yeah genie and purity i'm sorry it's
up here right it's not as computationally expensive to not to when you take out that log
conversion okay so as a result though it's this the outputs are slightly harder to to
understand relative to the zero to one scale but it's really not too bad um so here what
we see is you know kind of a a genie index here we're subtracting this from one that's
basically point five all right because we have you know just two and two here as compared to one
when we do when we did enthalpy here we see it's 0.5 which kind of represents a perfect
split okay i would do the same calculations all right generally we're just going to plug this into
our equation here and what we're going to see is that from this side we have 0.44 and this actually remains
the same as enthalpy you know kind of a one-sided dominated class would be zero
and then we're just going to do the same thing we're just going to subtract it from the parent right or just do a weighted average three out of four one out of four
all right not that it's going to matter because this is zero and then this is going to be our information gain okay uh
so it works very similar it just doesn't have this basically that logarithmic conversion component to it and as a result uh it kind of is the
default because it is a little bit more efficient right so we went through each one of these this would result basically in a
this is akin to the 0.33 that we saw earlier 0.31
0.31 that we saw earlier and if we did the perfect one it
would be 0.5 all right okay all right so
mean squared error is how it's done if you were doing um if we were doing a continuous
reducing basically a regression based tree all right and so
this is the equation for that it just tries to reduce the the total error of the predicted values at each node
right so the average of each of those groups in terms of the minimizes the mean squared error just
uses mean squared error at each point okay i don't have an example for that
but it happens very similarly to the way that we see um it occur
in with entropy all right it will do the calculations associated with predicting variables inside the tree
pick the threshold associated with that calculate the mean squared error associated with using that particular
variable at that particular split and then it will include the variable that reduces the mean square error the
most right at a particular threshold and then move on to the next node and do the same thing right
all right so here's another example here just to just ignore this up here oops all right just ignored this
we are going to go through this um uh this pregnancy tree example
in uh in class i keep skipping through this all right but the idea here is we're trying to figure out uh we're a uh we're a marketing firm
we're trying to figure out what our shopping shopper characteristics are trying to figure out whether someone is with child or not and so these are as
it turns out the purchasing of folic acid ends up being the best predictor so here we can see this is
kind of a pure split between this is this is pregnant and
that's the other way around sorry not pregnant and pregnant okay so these
are our shoppers that's our variable classification we're trying to predict you know kind of maybe the purchasing habits associated with our
with our clients and as a result then we can you know email them specials on diapers and vitamins or whatever that might be
okay but here what we see is that folic acid does a really good job 120 to 6 in terms of the ratio of pregnancy not
pregnant and predicting so that's why it's going to be our top one okay and then eventually we just work
through the tree gradually to try and understand
all right in particular on kind of the purchasing of non-folic acid to be able to better understand
the split between those two there's an arrow pointing here just to note out that this is a you know basically a leaf
node all right yeah which is kind of uh
high up in the graph okay all right so like i mentioned before decision trees are prone to overfitting
and one solution is we can tune those hyper parameters all right so another solution is built to ensemble talked about that
all right so these are just uh some examples of hyper parameter tuning that that we can
do all right you can set a minimum number of samples to be at a node split all right
go back to that 100 100 100 row example that we had maybe you would want to say hey i need to have
at least you know 10 data points in every node all right so that would avoid kind of
over spitting over splitting right minimum number of samples at a terminal node uh so that would be
um you know at what is the if you choose that same example up here we're saying we need at least 10 examples to do a split maybe the
terminal node you would say hey i need at least i don't know at least five examples to be in a terminal node or something like
that you probably want those numbers to be bigger maybe do 20 as a minimum split and 10 in the terminal something like that
and this is typically the way that people do it is they sept a set of maximum to the depth of the tree and depth just means the number of variables
that you're going to use and the splitting all right so if you have a depth of like five you're not going to have more than five splits
okay all right
you can also set minimums and maximums on the number of actual terminal nodes
okay so that's the number of you know the number of leaves at the very end okay all right and then
and the maximum number of features to be considered at each split this is another way that you can handle it all right and so the idea is that you know keeping in
mind when we have our we have our top number we're gradually doing all these splits
this is not all right have to go this way
all right all right
all right so as you move down this side of the tree all right the variables that
are available all right are ones that have not been used uh anywhere higher all right
inside the split criteria and it's the same for this all right so you'll actually you can see that maybe two
two variables a variable could could get used again all right it could be used twice inside a tree as long as it's not higher
inside the split criteria so in our example that we're doing before if oops you know say
you know this the first example was folic acid right that sent out this portion of the tree
in that portion of the tree folic acid cannot be used anywhere else inside the tree all right but say it was
vitamins what's the second criteria here vitamins could also be used
like over here all right so we might they might actually show up right next to each other in terms of understanding this
population that was in the non-folic acid in this population that was in the folic acid right so what you can do is control the
number of features associated to be considered at any one split so if you want to make sure that there are say five features that remain in order to
give your decision tree a rich environment with which to make decisions you can control that
and if it looks like the tree is complex enough to where maybe there's only you know two variables left to consider you could say hey that's enough let's
just stop there because uh you know the decision space is pretty low all right
okay so the we talked about trees all right this is
you know kind of a clunky transition here but we're moving into a different way to talk about talk about
training our machine learning algorithm and this is through cross-validation all right so the package that we're going to use for
decision trees actually defaults to this all right so it defaults using ten-fold cross-validation
all right so we talked a lot about training test all right the disadvantage between training and
test is you know basically they compete against each other all right as the training set gets
bigger all right testing goes down all right so there cross validation is a way to get around
that to use the entire data set all right and the training and the testing process all right so let's
let's talk about that all right so we have this tension that's why there's this rope here right all right
so uh basically you just select a k and typically it defaults to 10 10k full 10 fold
cross validation all right and so at every at every step what happens is is basically um the
algorithm will do this 10 times all right basically train say you set this criteria to i don't know what this
represents i don't i forgot the number of dots here right one two three four five six
anyway that's that that's there's let's assume that there's 24 dots here or something like that and so this represents 25
of the data uh and what you're going to do is you're going to train the algorithm with the 75 and then test it with this 25 and then
do the same thing here all right so train and test do the same thing here train and test
train to test you're going to do this 10 times all right so here this is basically 4 k
fold cross validation where they divided it into four equally sized parts and then they use
those parts in proportion to be able to train to test the data set all right
so uh we're going to do that too all right but the idea here is that uh you can use the entire data set in the
training process and then do basically internal evaluation with this with this holdout
and that allows you to use all the portions of the dataset as compared to just one big chunk for training
would be chunk for test all right all right so we'll i'll show that a bit
more as once we get into the code
okay so um let's take a look at overfitting all
right um again uh we've talked about well you guys know what overfitting is let's just
jump past that but these are some definitions that we threw in here kind of at the end so ensemble methods
uh we talked about that a little bit we're gonna talk much more about that next week but that's basically when you take you know many there's many different
forms of this but in this example instead of one tree but a whole bunch of trees together and they do majority vote
essentially all right all right so you know it is designed to operate
efficiently uh you know
but it does not guarantee that it can provide you the best the best model because it is slightly different
every time okay so let me stop there and then uh this is it's a lot to get through but
essentially the key key points here to remember about decision trees is that it's centered
on this idea of information gain they do obviously have a tendency to overfit so we want to be able to use those hyper parameters to be able to tune them
there's a lot of options to do that we're going to do a bit more technically about what cross validation
does and how it how we can use it in actual code operations it's a bit it's very handy especially
for especially for for decision trees and then also when we think about unbalanced
data sets uh that can be helpful for that all right because uh decision trees can't be
sensitive to kind of small changes and they're asynch all right so they grow out they're greedy they make these
little local optimized decisions but they're also very easy to interpret through
visualization and they're intuitive so they can be useful for all those all those types of reasons but
we will get much more into it uh in class i'm sorry it's a little bit longer than normal but
thanks for that and i will see you soon