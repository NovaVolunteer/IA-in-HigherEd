okay all right so let me pop this up herebigger i can do that there we go all right so we've been talking about decision trees today all right we'll build alittle bit maybe towards ensemble models towards the end but principally we're just going to talk about decision treeswe're going to talk about how they're made or give you some theoretical background related to uh rates of how they are how constructedand how they're used they can be used for both regression and classification we're principally going to talk about classificationat this point which is the primary use case for them and then um i'll talk about some of the weaknesses and strengths generally all right solet's just jump right into it here all right okay so this is what a littleahead of myself trying to shrink my there we go okay okayso uh i said all of that already so all right so let's talk right awayabout uh about the way that trees work okay so when we think about them they're kind of this you know this upside down trianglebasically they kind of build out this way and they're constructed largely of these uh you know basically thesefour things right so we have a root node which is basically this top note up here in a decision tree they have these internalnodes which are basically these split nodes they come out like this all right so it has one edgeand these edges are just basically the lines that are connecting to connecting the nodes okay and then theyhave a leaf node or terminal node kind of the same thing it has one node coming in one edge and then no outgoing edgesbasically you think about them as kind of at the bottom of the tree okay so this is our this is our terminali'm sorry our root up here this is an internal node and then these are the leaf nodes andthen all of these things together have these edges that connect them all right hey it's really easy just to look at it thisway and so here what we see this is our this is our root note up here all right this is the old classic irisiris data set that everybody just loves seeing over and over again so here we have the root nodes these areinternals and then down here on the bottom we see the leaves all right so this is amatplotlib graphic actually coming out of python itdoes a bit better job and we'll talk about kind of this all the components of this as we move along so what's going oninside the boxes i don't have to totally understand but uh we'll take a look but this isalso important note here is that right so a leaf node doesn't have to be all the way down on the bottom all right then we have one right herebecause we have no edges coming out of this basically what this is saying is thatpetal width just to give you some context here pedal width does such a good jobof classifying setosa right that's remember that's a type of flowerthat we have there does such a good job of classifying setosa there's no reason to move on as you can see here basically any anypetal width that is less than or equal to 1.75all right is going to be fall into the subtosa class right essentially all right so let'skeep moving all right so that's that's it at a very high level is what we think about whenwe we think about decision trees and again it's just like this a cyclical graph that kind of moves down all right buthow do they make these choices so let's let's talk about that all right so the most important question so whatis the most important question uh to move on to a second date right it seems to melike uh you know are you married is probably a more important criteria than uh heywhat's your favorite music you know if you don't know this one i guess it depends on your youryour ethical standing but yeah in general you have to say to yourself what are these important questions thatthat you need to ask or need to be informed of in order to move on to a second date all right sowhen we think about that the question that the most amount of relevant information this is exactly how a decision tree works allright they're going to need they want to be able to figure out which variable gives it the most relevant informationto be able to decide uh how to classify our target variable all right all right soyou know depending on the condition of the first answer all right then you might select you know the next most importantquestion for in terms of just information gain as maybe the question we're trying to work out here obviously isyou know whether we're going to move to a move to a second date all right so are you married all right if the answerto this question is yes hopefully most people would say whoa okay well check pleaseall right so that that might be it all right if the answer here is yes is yes all right again you would stopall right but then you know the question would be like what is the next series of questions that you might askall right you know belief in a blue colored sky you know maybe that's good because you could you knowget a sense of whether someone is insane or not or you know maybe then you do move on to say you know do we have more things in common youknow what are your favorite foods or music or whatever all right so if you if the answer was tono then maybe you move on to the next variable which talks about something that is just kind of more personal to see ifyou're you know if your likes and dislikes overlap okayany question is like when should you stop asking questions all right so say you made it you passed the uh past the marriagehurdle and now you're on to what music do you like right and sohere you know it might be that you know a certain person you know fifty percent will go on uh ona date if they believe that they like you know acoustic and then maybe this this is disco or something and so you know 50percent of people will go that so then you start getting very specific about likes and dislikes in terms of what typeof music alright so you can imagine this would be just a factor level variable that could have all sorts of different types ofclassical and brock and you know the rap andlots of other types of music which apparently i'm having trouble naming but anyway so you think about that anddepending on the answer to that maybe people would have more information about whether their likes or dislikes overlapped right okay all rightso this is basically how this how this works okay so uh it's fairly intuitive that's a bigreason that people like decision trees because of this nature you can kind of explain it in simple terms about how they're used andhow they make choices all right so we asked the question you know what iswhat is the variable that has the most important information in terms of the ones and zeros that we're trying toclassify and here we're trying to classify whether someone is likely to go on a second date or not all right so weasked this first question about marriage all right and that results similar to what we saw in the flowerexample to maybe a perfect classification of whether they will go on to the second date or not right the majority of people would notgo on to a second date right knowing that someone was married but if the answer to this is no all right this would be no this would beyes and then up above here we basically would be you knowsecond date is the variable we're trying to classify right so that would just be a list ofzeros and ones right zero being no second date and one being yesokay excuse me and then we get more specific right based off the type ofmusic that people are interested in there might be more of a disjointed resultright okay so we asked the most important information and then conditioned on the first answerwe select the next most important question and when the answer is no longer providing any additional information westop growing the branch all right it looks like maybe this is a 50 50 split you know 50 50. something like thatwe kind of stopped growing the blanche all right and we just do this all right and we'll talk about stopping criteria it also has a lot to do withthe complexity perimeter things like that or hyper parameters that you can tune to keep it above a certain levelall sorts of different things you just basically keep repeating these two steps over and over again until no moreinformation can be gained with the variables that you have all right so you use all the variables thatyou have inside your inside your data set to try and gradually get to know better and betterwhether someone will go on a second date or not all right okay all rightso decision trees are hierarchical all right what that means is basically umthey're built just in the way that you know we've kind of been viewing them is that they you know they start start small and growmore complex all right decisions decisions are made until a predetermined metric is met and we willtalk about what that metric might look like all right it could be you know a certain amount ofterminal nodes all right predefined there's lots of different hyper parameters you could actually set a lot of people set thenumber of layers say if you have you know a huge amount of data at your disposalyou know the more layers that are in a tree the more complex the decisions get all right all right and sothe model is is built such that a sequence of ordered decisions concerning the value result and you know at the endthe model inside the terminal nodes are the leaf nodes all right actually predicts the class label okay i'll giveyou a percentage likelihood of that as well so it does it has a probabilistic output similar to k n it's non-parametricuh it just means that you know the number of parameters is not determined all right as is the case with linearmodels you know you kind of know what the variables are going to be with this it's it's not alwayspredestined right you could have a couple of variables that are incredibly good at predicting these classes you know marriage might be oneof them and so you actually wouldn't need that many to have a very high level of certainty or to hit some level ofmetric that you're targeting or it could be that there are you have a bunch of weak variables so you need to have a very complex tree tounderstand how people's decisions are are being made right and so we also don't need to worry aboutany assumptions being met about distributions or perimeters okayso there's no normality assumptions no standardization all right it's just a cyclical graph allright so it's used in model probabilities and causality you can use there are causal trees a bit morecomplicated to translate but trees consist of these nodes and edges and they're defined by this decision ruleand we're going to we're going to talk about what that rule is all right okay let's talk about the background alittle bit all right so it uses recursive binarysplitting which basically means that it it considers every possible partition spaceand then it divides the one based off uh it chooses the partition space basically chooses where to splita variable to locally optimize that information gain right so it doesnot consider the future of the tree doesn't consider the past it only tries to optimize thatindividual node those individual internal leaves okay all rightand it will consider every possible partition uh available when doing that okay so itthis is this is what we call kind of a greedy approach right it's kind of like localized optimality um optimalityoptimization inside that one internal node okay so it's going to try and make the best choice it canuh inside that internal node right so it's each step of the tree buildingprocess it picks the best the best split for that particular that particular split it does not considerthe future all right all right so trees you know i talked about this they can be they can use be used for um predictionor classification but they use recursive binary splitting in both cases all right um the difference betweenregressive trees uh is that they're they're going to predict okay so classification trees are goingto predict the actual class whereas regression is going to predict a particular number all rightright okay so the probability measure um that derives this building forclassification comes in two forms all right so we have these these two um measures which aregenerally used for are very typical it's either it's either a genie index orenthalpy genie index is actually a bit more common you saw that on the on the graph that we showed earlier with the iris data set it actually gave youthe genie index at each internal node but we're going to go through both of them they're very similar they're bothrelated to basically to information gain which is the key component to kind ofunderstand how decision trees work is this idea about that they're they're selecting the variable and how to split thatup variable based off the idea that they are maximizing information gained locally at that particular node okayall right all right so the background here this isthe cart algorithm all right um it's a classification and regressiontree all right it was first uh introduced here by 1984 with these fourresearchers all right it can be used in numerical categorical data all right so the first or splits thetraining data into two subsets all right i guess we kind of know that a single feature all right it searches through all the possible featuresall right to identify the split that produces the purest subsets right so the the purest idea is that purity isrelated to maximizing information gain okay all right so it stops once you can't find a pla a split all right thatthat reduces impurity right or by some other pre-predetermined hyper parameterall right so let me kind of explain a bit more as we go all right itall right so that's a bit of a theoretical background of those two papers it really just takes kind of this idea of information gain and puts it intopractice so it's similar to those four steps that we talked about the card algorithm is basically those four stepsin practice you know we're repeating steps two and three all right okay it does have uh some prettysome pretty weighty advantages right it is simple to understand and interpret through data visualizationthis is a single tree remember you just produce a single tree all right it doesn't require almost any data preparation at all all right theother techniques often require normalization or dummy variables need to be created and blank values needto be removed you know trees have a lot of advantages that they do not have to do a lot of that all right so they're able to handlenumerical categorical data all right it uses it what we call a white box bottle all right so you can actually see what'sgoing on all right you can understand why it made the decisions it's made and uh it's very interpretable okay ascompared to maybe more black box models some neural network approaches even though we're getting better at explainable ai things like that umand then even when you get up to ensemble models with random force they also become a little a little hard tounderstand it's pretty straight straightforward to evaluate i mean most of the models nowadays are not too bad you can usethese the evaluation metrics that we went over um you know in you know lastweek in the week before adding you know edit at a pretty high rate uh pretty much for all of these butthey're also very usable here for tree based methods um okay okay it does have somelimitations as all algorithms do so this is important to remember what they are otherwise uhwe'll be using to use uh use these methods in the wrong circumstances all right they do havelike a legitimate uh tendency to overfit you know if you're just growing a treewithout any type of restrictions you're almost guaranteeing that it's going to overfit linear models are the same wayso we have to control for those a bit all right so you have to think about you know if you have 100 you know if you hunt as an example ifyou have 100 data points right you have six levels you're gonna have 64 terminal nodes right if you only have 100 data pointsso you basically you don't have a lot of single a lot of single terminal nodes which means you're basically just memorizingthe data they're not going to generalize well and this is again this is our bias variance trade-off we'd want to use hyperparameter two need topull that back up or maybe some other uh metrics that we'll talk about in particular this this this idea of this complexityparameter to set that to a certain level that minimizes the error associated with the predictions and the actual valuesand that would be the metric that we try and target so there's lots of ways to be able to control this so it has this problemwe also can control it all right they can also be generally unstable tosmall variations in the data because of the nature of how they're how they're designed all right sothis is why you know a single decision tree depending on how good or bad your data is can be veryuseful but the truth of the matter is that often people build towards these ensemble models which likerange random forest which is just you know thousands hundreds or thousands of treesto be able to compensate for kind of this instability associated with a single treeall right all right so practical distribution tree algorithms are based off of you knowheuristic which is a greedy we talked about this they make local optimal decisions all right so they can'tguarantee that you'll get like the global global optimum all right similar to uh someother methods like this is that it could be possible that you know you don't get the perfect tree which again is a reason to movepotentially towards ensembles all right all right so the great uh bias trees ifsome classes dominate it is there for recommended that you try and balance the data set prior tofitting all right so if you have a heavily unbalanced data set and this is true of many of the classifiers we're going to be talking about it just doesn't performquite as well all right support vector machines actually have a tendency to work a little bit better as an example forunbalanced data sets a single tree can can be can be can be problematic with data sets that arequite unbalanced but again that can be cured by moving to kind of more of an ensemble approachbut until we get there we have to understand what one tree is doing before we start building forests all right so let's let's take a look allright these mathematical approaches and i have some examples all right okay so decision trees uh use severaltypes of node splitting criteria all right so these are these are those i've talked about theseuh this is what it would be if we had you know kind of a continuous data problem we're trying to actually predict the value all right ascompared to predict the class and these are the two common ones for classification they're really verysimilar all right we're going to take a look at we're actually not going to take a look at this one but we're going to take a look at each one of theseand they both use this idea of information gain to determine variable split criteria allright so let's what is this crazy information gain thing all right so this is enthropyall right all right so that's the that's the umthat's the equation there all right so basically it's just this all right so it's the the probabilitytimes the log probability of a particular instance you're going to summarize thesetogether all right so just let me let me walk you through this instead of detailing through the equation hereall right so what we would see here okay so this is we have a total set of six allright so we're thinking about basically this is our equation inpractice here just this portion of it so we're going to sum these two up and then we're going to subtract them from each other and then that's going to give us basethe general enthalpy of these six dotsall right and you can see here what we have is basically perfect classification all right so or maybewhere we have uh three and three all right so when we subtract these two things from each otherright we just get kind of the net entropy associated with this example right we do the same thing herewe have a little bit of more discontinuity right so we have four and two okay so we just plug andchug basically into our equation we're just doing the probability here which is two out of sixall right and then probability here so four out of six we're just multiplying that by thelog two of exactly that same ratio and then subtracting them from each other just to get an idea of the balance righthere so what we see here right it's just a total you know kind of one class example obviously we we don'thave to do the blue calculation because we know it's zero and so we just do the green side of it which is going to result in basically noinformation so you can so we can put these let's extend this a bit further and just kind ofclumsy through that all right the idea here is that information gain is basically going to be right theentropy of the parent right which is what we did up here this is the enter fee think of this as the calculations of theparent all right this is the by the parent i mean before the split all right so this is just what our data looks like before webefore we use any variable examples to be able to split it rightand then it's going to be the minus the average entropy of the children right and the childrenare going to be what it looks like you know after some type of decision criteriahas been made okay so you can think of this as up here what we have this is entropy theparent so we have kind of a perfect division here which is which is which is good maybe that's what we want umyou could think of this as you know pluses maybe went on the date right so that's a itwent on a second date just to continue to extend this exampleand the dots all right did notokay and then we're gonna then we apply some type of question right so this might be i don't know thismight be the marriage question right soall marriage just pretend i know how to spell all right and so and this would be maybe this is yessorry this is a yes category some people are married all right so as a result umwhat we see here is that if the answer is yes all right that two of these uhtwo of these dots stop right there all right so that's an entropy it's a perfect classification relative to all right no date all right so see nodate which makes sense all right so they yes all right the people the no date uh stop right there all right but but oneperson you know this is uh so no all right so despite thatthere is some uh all right blending of that and it could just be that you knowwhatever that might be is that there's there's no date here all right so what we see isthat there are four out of the all right original six continue onand probably what we see here in this green dot is maybe there's some other reason why that second date didn't occur all right so we'd have to continuemaking splits but for this there's no there's no additional reason for us to continue here right because we havebasically a terminal node okay so we've classified this perfectlyassociated with this one no date class and that's exactly what we talked about kind of in the beginningabout how good a particular question would be but here all right we have some we havesome blending right so we don't see a zero so we wouldn't stop we'd continue to go okay all rightnow in order to be able to calculate this and just erase some of this trying to put this in context all rightwe're gonna take the average of these two numbers right so the average of 0.80.0 all right and we're going to subtract that from the parent and that will tell us the qualityessentially of this marriage question or it would give us the net effect in terms of information gain for thisparticular question at this particular partition all right so this is a is a yes no sothere's just the partition is simple it's just did they get did are they married or not but if it's a continuous variablebasically try uh every type of potential partition inside that range ofvalues calculate the information gain to see where it makes the most sense which is the example we saw for umi guess it was pedal width right so at a certain numerical range 1.75or greater or was it less anyway at a certain threshold associated with that that distance for that pedal theyhad perfect classification for that flower so that's where that um split criteria occurred okay so itshows all the different potential ranges inside that basically all the the the potentialpartitions inside that continuous variable that are represented in the data frame all right they're represented in that vector and chosethat one as being the best because it provided almost perfect information for that particular flower all right so what we would do here thisis a now it's a weighted average because it takes into consideration what the originalsplit was right okay and so we just subtract uh we add these two things together allright and then we just subtract it from one and so we get 0.54all right okay so that would basically be our our net gain there associated with umassociated with our uh information information gain all right so what we would do to finishthis is what we see here is this .54 is the result of this weighted average remember that this isour calculation of our enter fee previously we just add these together0.54 then we would subtract that from one and then here we would say we have a general information gainof 0.46 okay all right so now keep in mind if it wasperfect all right and this resulted in a perfect split as in the extreme examples which sometimes we'd like to refer to thatthis would basically just be zero right and then we'd have you know one minus zero we'd have a perfect information gainof one sorry about that all right and so we would we wouldn't have to moveon any further we'd have a variable that perfectly classifies into date and no date as an exampleright but we don't have that all right it's not a perfect classifier it's somewhere in between it's perfect for this one side but there's still lots of informationthat could be gained uh and specific to to the um this is the you know when on a datecategory so we need to continue to continue to grow this tree out so we could better understand that particularside of the uh side of the equation okay so let's take a look at anotheranother example here all right so what we see in order to start the tree right we need to follow again just thesethese basically these three steps right i keep giving you steps here but all right we choose a attribute with thehighest information gain okay so we can check child nodes and then we repeat repeat steps one and two you know we'recursively into uh no more information can be gained all rightso let's take a look at that all right so this is a pretty common example that gets used but whatwe're trying to do here is to predict you know should we play outside i think the original example here is like shouldwe play tennis outside i think are we going to be able to play tennis something like that all right so we have these three variables right this is ourpredictor right we're trying to predict um whether play happened outside or not and we havethese variables to help us like the outlook you know the temperature and then the you know the humidity all inclassification all right all in um factors right so let's take a look so this is our ouroriginal variable all right so when we look here yes and no all right that's our play so we have a perfectall right we've got a perfect information perfect entropy here here we had two that went and played andtwo that did not so that that's our parent node at one all right and then we're taking a look at this firstvariable here sunny so that resulted in one yes all right all rightand uh two nodes two nodes all right so we have three sunny variables allright and then when we look across what we see is we have two nodes and one yes all right so it doesn't giveus perfect information all right but it does give us some pretty good information that when it is sunnywe have one yes and two nodes all right but then we have you know when it's cloudy which is alsohere what we see so we have one yes all right so we're trying to figure outplay or not play which is at the top and the first variable that we took a look at all right we haven't determinedif this is the best variable for us yet but the first one that we took a look at is outlook so we just did thiscombination again we see a zero here we see a 0.92 here and then we're justgoing to do this weighted average and then subtract it from our originaloriginal entropy of our target variable okay so this is the weighted averagewe're just going to do this 3x4 because remember we had threethree sunnies and one cloudy all right i'm just going to multiply that by the by our enthalpy gain here which is justthis formula that we talked about earlier and we're going to add that to the other one which is going to begoing to be 0 right okay and then we're going to subtract it by 1. so we're going to get all right 0.31okay so let's keep going let's evaluate let's do this same process and we'll look at some of the other variablesall right so let's look at hot and cold all right so what we see here all rightwe see temperature if the temperature is hot right no play no play the temperature is coolit did play and they did play all right we're giving it away here but this is going to result in perfect classification all right so what we'regoing to see here all right is that the relative to the entropy uh that was seen in the target variablewhen we do this weighted average obvious this is all going to be zero just like i said before it's going to result in a perfectperfect information gain okay so here this perfectly matches ouroutlier or perfectly matches our target variable whereas the example we had before didn't itwasn't quite a perfect split right we had one yes over here and we have one yes over here so when we calculateinformation gain it's not quite as good as when we do it here right we have perfect information gainall right so let's we're talking a we've seen an in-between example we've seen a perfect example so now let's look at aperfectly terrible example all right so here what we see is we have perfect disagreement okayregardless of whether it's high or low it looks like there was a decision to play and to not play all right so here wehave one yes and one no on both sides of this highlow equation so what we're going to see is when we plug this into our uh you know our enthalpyequation and we subtract these from each other uh we're going to get one all right which which is basically meansthat you know we're not we don't learn anything all right the result of this is one restrict one from oneour net information gain here is going to be absolutely nothing okay so if we were to build thisclassifier uh what we would see here this is this is very uncommon of coursebut what we see is that our we have this temperature oopstemperature would be right at the top all right so that would be the first variable that we would use it providesus the most information and then the next one would be outlook i mean actually technically we wouldn't even move on to a next one because wehave perfect classification here so we wouldn't even need another variable so in this in this case when you have perfect information gainum you just have a you know a tree with the depth of just one split and that's it okayso that's enthropy all right so information gain is is very similar all right all right so it basically usesthe same idea about net gain all right so pi in this equation just representsthe probability that a random selection would have state i whether it would have you know you think it was whethersomeone would be would play or not all right and so the mathematical process thisis pretty much the same i don't think i put the actual i thought i had the actual equation in here butuh anyway you kind of get the idea here so it's this is just the same all right this is the example that we saw beforeall right relative to what the outlook was like right so thisis the outlook calculation it's this one right here okay so we use the samesame general idea i'm just going to show it using the genie coefficientthe difference is that there's not it does not actually include this log conversion okaywhich uh is is most of the reason that uh that most of the packages and uhtechniques associated with uh information gain and this uh criteria for selection have atendency to default to gini to genie index because it's just not as computationally oh yeah genie and purity i'm sorry it'sup here right it's not as computationally expensive to not to when you take out that logconversion okay so as a result though it's this the outputs are slightly harder to tounderstand relative to the zero to one scale but it's really not too bad um so here whatwe see is you know kind of a a genie index here we're subtracting this from one that'sbasically point five all right because we have you know just two and two here as compared to onewhen we do when we did enthalpy here we see it's 0.5 which kind of represents a perfectsplit okay i would do the same calculations all right generally we're just going to plug this intoour equation here and what we're going to see is that from this side we have 0.44 and this actually remainsthe same as enthalpy you know kind of a one-sided dominated class would be zeroand then we're just going to do the same thing we're just going to subtract it from the parent right or just do a weighted average three out of four one out of fourall right not that it's going to matter because this is zero and then this is going to be our information gain okay uhso it works very similar it just doesn't have this basically that logarithmic conversion component to it and as a result uh it kind of is thedefault because it is a little bit more efficient right so we went through each one of these this would result basically in athis is akin to the 0.33 that we saw earlier 0.310.31 that we saw earlier and if we did the perfect one itwould be 0.5 all right okay all right somean squared error is how it's done if you were doing um if we were doing a continuousreducing basically a regression based tree all right and sothis is the equation for that it just tries to reduce the the total error of the predicted values at each noderight so the average of each of those groups in terms of the minimizes the mean squared error justuses mean squared error at each point okay i don't have an example for thatbut it happens very similarly to the way that we see um it occurin with entropy all right it will do the calculations associated with predicting variables inside the treepick the threshold associated with that calculate the mean squared error associated with using that particularvariable at that particular split and then it will include the variable that reduces the mean square error themost right at a particular threshold and then move on to the next node and do the same thing rightall right so here's another example here just to just ignore this up here oops all right just ignored thiswe are going to go through this um uh this pregnancy tree examplein uh in class i keep skipping through this all right but the idea here is we're trying to figure out uh we're a uh we're a marketing firmwe're trying to figure out what our shopping shopper characteristics are trying to figure out whether someone is with child or not and so these are asit turns out the purchasing of folic acid ends up being the best predictor so here we can see this iskind of a pure split between this is this is pregnant andthat's the other way around sorry not pregnant and pregnant okay so theseare our shoppers that's our variable classification we're trying to predict you know kind of maybe the purchasing habits associated with ourwith our clients and as a result then we can you know email them specials on diapers and vitamins or whatever that might beokay but here what we see is that folic acid does a really good job 120 to 6 in terms of the ratio of pregnancy notpregnant and predicting so that's why it's going to be our top one okay and then eventually we just workthrough the tree gradually to try and understandall right in particular on kind of the purchasing of non-folic acid to be able to better understandthe split between those two there's an arrow pointing here just to note out that this is a you know basically a leafnode all right yeah which is kind of uhhigh up in the graph okay all right so like i mentioned before decision trees are prone to overfittingand one solution is we can tune those hyper parameters all right so another solution is built to ensemble talked about thatall right so these are just uh some examples of hyper parameter tuning that that we cando all right you can set a minimum number of samples to be at a node split all rightgo back to that 100 100 100 row example that we had maybe you would want to say hey i need to haveat least you know 10 data points in every node all right so that would avoid kind ofover spitting over splitting right minimum number of samples at a terminal node uh so that would beum you know at what is the if you choose that same example up here we're saying we need at least 10 examples to do a split maybe theterminal node you would say hey i need at least i don't know at least five examples to be in a terminal node or something likethat you probably want those numbers to be bigger maybe do 20 as a minimum split and 10 in the terminal something like thatand this is typically the way that people do it is they sept a set of maximum to the depth of the tree and depth just means the number of variablesthat you're going to use and the splitting all right so if you have a depth of like five you're not going to have more than five splitsokay all rightyou can also set minimums and maximums on the number of actual terminal nodesokay so that's the number of you know the number of leaves at the very end okay all right and thenand the maximum number of features to be considered at each split this is another way that you can handle it all right and so the idea is that you know keeping inmind when we have our we have our top number we're gradually doing all these splitsthis is not all right have to go this wayall right all rightall right so as you move down this side of the tree all right the variables thatare available all right are ones that have not been used uh anywhere higher all rightinside the split criteria and it's the same for this all right so you'll actually you can see that maybe twotwo variables a variable could could get used again all right it could be used twice inside a tree as long as it's not higherinside the split criteria so in our example that we're doing before if oops you know sayyou know this the first example was folic acid right that sent out this portion of the treein that portion of the tree folic acid cannot be used anywhere else inside the tree all right but say it wasvitamins what's the second criteria here vitamins could also be usedlike over here all right so we might they might actually show up right next to each other in terms of understanding thispopulation that was in the non-folic acid in this population that was in the folic acid right so what you can do is control thenumber of features associated to be considered at any one split so if you want to make sure that there are say five features that remain in order togive your decision tree a rich environment with which to make decisions you can control thatand if it looks like the tree is complex enough to where maybe there's only you know two variables left to consider you could say hey that's enough let'sjust stop there because uh you know the decision space is pretty low all rightokay so the we talked about trees all right this isyou know kind of a clunky transition here but we're moving into a different way to talk about talk abouttraining our machine learning algorithm and this is through cross-validation all right so the package that we're going to use fordecision trees actually defaults to this all right so it defaults using ten-fold cross-validationall right so we talked a lot about training test all right the disadvantage between training andtest is you know basically they compete against each other all right as the training set getsbigger all right testing goes down all right so there cross validation is a way to get aroundthat to use the entire data set all right and the training and the testing process all right so let'slet's talk about that all right so we have this tension that's why there's this rope here right all rightso uh basically you just select a k and typically it defaults to 10 10k full 10 foldcross validation all right and so at every at every step what happens is is basically um thealgorithm will do this 10 times all right basically train say you set this criteria to i don't know what thisrepresents i don't i forgot the number of dots here right one two three four five sixanyway that's that that's there's let's assume that there's 24 dots here or something like that and so this represents 25of the data uh and what you're going to do is you're going to train the algorithm with the 75 and then test it with this 25 and thendo the same thing here all right so train and test do the same thing here train and testtrain to test you're going to do this 10 times all right so here this is basically 4 kfold cross validation where they divided it into four equally sized parts and then they usethose parts in proportion to be able to train to test the data set all rightso uh we're going to do that too all right but the idea here is that uh you can use the entire data set in thetraining process and then do basically internal evaluation with this with this holdoutand that allows you to use all the portions of the dataset as compared to just one big chunk for trainingwould be chunk for test all right all right so we'll i'll show that a bitmore as once we get into the codeokay so um let's take a look at overfitting allright um again uh we've talked about well you guys know what overfitting is let's justjump past that but these are some definitions that we threw in here kind of at the end so ensemble methodsuh we talked about that a little bit we're gonna talk much more about that next week but that's basically when you take you know many there's many differentforms of this but in this example instead of one tree but a whole bunch of trees together and they do majority voteessentially all right all right so you know it is designed to operateefficiently uh you knowbut it does not guarantee that it can provide you the best the best model because it is slightly differentevery time okay so let me stop there and then uh this is it's a lot to get through butessentially the key key points here to remember about decision trees is that it's centeredon this idea of information gain they do obviously have a tendency to overfit so we want to be able to use those hyper parameters to be able to tune themthere's a lot of options to do that we're going to do a bit more technically about what cross validationdoes and how it how we can use it in actual code operations it's a bit it's very handy especiallyfor especially for for decision trees and then also when we think about unbalanceddata sets uh that can be helpful for that all right because uh decision trees can't besensitive to kind of small changes and they're asynch all right so they grow out they're greedy they make theselittle local optimized decisions but they're also very easy to interpret throughvisualization and they're intuitive so they can be useful for all those all those types of reasons butwe will get much more into it uh in class i'm sorry it's a little bit longer than normal butthanks for that and i will see you soon
