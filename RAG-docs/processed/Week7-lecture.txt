okay hello welcome to uh week 11's lecture uh it's been a little while since wetalked about some of these things we had the kind of uh with the package presentations in the middle but we're going to talk focusmostly on supervised machine learning we're gonna get into uh reallyspecifically uh well anyway that's the kind of topic for the evaluation metrics i'm gonna review some of the stuff that we went overum in terms of just how to deal with the output from these kind of probabilistic models and we're gonna translate thoseinto much more discrete terms in terms of how we use them just to evaluate our models so that's what we'redoing this week it's all uh it's all uh model evaluation if i if i had to be quite honest youknow outside of building models feature engineering you know those types of things model evaluation uh is kind of criticalacross the entire lifecycle data science projects you don't just do it once you do it almost continuously so having agood grasp on the different ways you can evaluate a model is going to be criticaluh to understanding anything about machine learning uh i think actually to be honest it's probably a step that gets overlooked alot we're gonna spend a whole week here talking about it and then we're gonna go back to our k n models we're gonna think about different ways toto evaluate them all right in the lab so let's jump right into it shrink myself here okay all right sojust as a review here what we've got is uh uh this is our typicalkind of you know well anyway machine learning kind of view here i'm just going toangle this just a little bit further like that there we go uh and so what we see is we got our you know we got our data coming in we got toput it down here in the test set put it over there the training set we talked about that all right so we split into training and testall right and so uh what we're gonna do is we're gonna build our models off of this training set all right so that to a certain extentyou know there's no uh there's no biasing our training set um when we go to evaluatewith our test set so we're going to build that model off of the training set this is all just kind of a review hereall right and then that bottle is going to be able to to uh make some predictions right we're going to have this modelbut what we're going to do is we're going to take that test set right we're making this more confusingit has to be and we're going to pass that through our model we're going to get our predictions but then we're going to do evaluationsright on those predictions okay so we can we can evaluate how well ouractual model works all right we can do that you know we can run evaluations insidejust based off the training set we can do that all right we can basically detect our error rate things like that you know because we're going to havelabels for this training set as well but the idea primarily is to evaluateonce we built our model on unseen data which is how we're going to use that's why we do training testso that's what we're going to talk about mostly and that's you know the the enormous approach when we thinkabout how machine learning works okay all right this is just the same thing again i don't know why it's there againbut it is okay so the question really is what metrics should we be should we be using allright accuracy is the one that everybody talks about it's almost always not enough you knowso how reliable are the predictions and what are the errors of our training dataare they a good ins a good indicator of future area uh errors i'm gonna tellyou right here the answer is probably not right to a certain extent they they will but it's important to know that we haveto be thinking about just basically continuous evaluation all right you know data drift is the thing whereyour data starts to change over time so using the same model the results may become skewed or biasedall right so that's why you have to continuously evaluate once you put these models into production we always have to be thinking we're notjust building something in isolation what's it going to look like when it's actually working out there in the real worldthat's why model evaluation is so important because we have to keep track of that okay so they're going to be futureerrors in your data future changes with it uh there's 100 certainty that that's going to happen nodata stays all the same so we have to continuously be evaluating performance that's why it's it's so important it's a big thing thatdata science professionals do is just evaluate how these models are working and if they'renot working the way they're supposed to be then they get retrained right okay so we talked about this we have our kind of our four quadrant modeluh four quadrant kind of concept here with full seconds or circle stuff and then erase a lot so true positivesfalse positives true negatives and false negatives all right i'll let you read through this but you get it which is basically analogizedto like the uh to the wolf scenario all right so if there is a wolf and we say it's a wolf true positive allright if there is no wolf when we say there is no wolf true negative and the opposites are true here okay what we're talkingabout there is a confusion matrix all right we introduced this two weeks ago but the confusion matrix is basicallythe foundation with which we're going to build all of these evaluation metrics around it's how well our modelin a classification standpoint all right so we're doing all classification here i'll talk about regression towards the end but this isall classification all right how well our model is doing in putting these uh putting our training data or otherwise into these specificcategories okay so we do that using this this confusion matrix and it's aptly namedyou know it's fun to joke about this but it's often confusing all right and that's going to generatesomething called a receiver operating curve and an area underneath the curve all right we're going to get into how those are calculated but they usesomething called a threshold all right threshold we'll talk about that later basically it's going to usethese different threshold measures all right and calculate the accuracy at each one of those measures right andthat creates this curve i'll show you how that works all right so in keeping in mind the way thatthings work is that classification models you know typically all right they're looking for a specific classa one or a zero right that's a bullion output you know kind of a binary classification problem where one all right is going to be thepositive attribute right typically what we mean by positive attribute hereis that there is some benefit to being in the positive class right it's usuallywhat it means and it can be translated into different ways but the ones we're going to show here too is for instance the identification of apotential threat to a system might be a one right that's a positive outcome that we positively identified a threatso that's the reason that we did it that way all right but typically when we think about this thing the example people talk about is like getting access to some resource rightyou got you got the loan right you got into school whatever that might be that's usually uh what we think about wethink about the positive attribute in terms of classifying different people okayall right so here we'll look at that all right these are uh probabilisticoutputs so the idea is that they're they can't they'll predict hard classes right these ones are zeros but the waythat they predict these classes is based off a percentage likelihood of that event to occur allright so we'll i'll explain exactly more what that means but uh just really quick i mean the idea isthat for instance if a model predicts that something is 75 chance fraud it's likely thatthe output of that will be a one right if it predicts that there's a 25 chance that it's fraud the output ofthat will be a zero all right i'll get into more details about that but generally that's how it works you know a higher percentagelikelihood of something it's going to predict it it's going to put that one label on it less likely it's going to put a zerookay all right all right solet's work through an example we're talking here about intruder fraud detection all right not exactly a super exciting example butyou know we're going to get in there so the idea here is that you know we got 135 emails all right they're entering our ourgoogle account right we're going to try and detect whether some of them are spam or not classic spam no spam situation here allright and so what we're going to do is we're going to generate a you know a theoretical you know a magical uhprobabilistic measure a result of a tree based classifier all right to determine the likelihood that any one of these emailsis fraudulent all right so we're generating probabilities here so percentage likelihoodall right the cut off point all right this is important here right the cut-off pointthat is predetermined in the tree and is a universal standard of most machine learning algorithms is 50okay all right so what what does that mean all right so justthink back to what i just said about the 75 25 scenario all rightand let's be more discreet about that all right in this example right if ouruh our tree based classifier don't worry about understanding what that means just we just replace it with k n if that'seasier to our classifier just take that part off our classifier predicts that one data point right let's just saythis is data point x right the likelihoodprobability of that particular data point being spam is 51 rightso what label is it going to put on that you guessed it right this isn't thathard it's going to put a one on it if there's another data point you know data point y whatever that isi guess i could do like x1 and then you know x2 something like that 49 right it's going to predictthat's a zero that's what we mean when we talk about cut off all right this cutoff all right it'salso referred to as the thresholdit's a threshold measure okay these are the same thing cut off threshold generally they'retalked about in the same breath okay and so for this instance the threshold is 50 so anything equal to or greaterthan 50 gets classified into the positive class all right and anything
less than fifty percent is classified into the negative class and to the zero all right and that typically is the the
uh the extent of the goal of most machine learning algorithms is just to create
this classification system it's just that simple okay all right most of this i think we kind
of know but let's walk through this example right below are some results of this really down here of our
of our model all right our in our very confusing matrix okay and so here um uh you know they center
on positive and negative classification i don't think i need to read through all this let's just look at this right so both true negative
and true positive are good all right so we want these numbers here we've talked about this before to be high we want these
to be low okay so in a perfect model right all our data points would be in here there's no such thing as a perfect
model if you see that that's happening something's probably going terribly wrong but anyway important to think about it
all right as a goal okay so here we're walking through the idea of thresholding all right so we're
not just showing the confusion matrix we're thinking about how we can use that threshold all right
to our advantage or disadvantage to better understand how the classification is working all right this is another
uh it's another piece of machine learning that you can change all right you can attenuate
in production to fit your research problem or your whatever project problem you happen to
be working on all right adjusting the threshold it's another knob it's another lever that you can use okay so here think of
this as this you know is this is this crank right we've turned this threshold all the way down to zero
all right so what does that mean okay what that means is that everything is captured as fraud all right and nobody
ever gets an email again and everybody lives happier happily ever after right okay so what we see here
right is that because our threshold is so low right is that we
have no tolerance for potential fraud okay everything gets labeled as
potential fraud so what happens is that we have 103 false positives here because remember
everything's fraud all right but we did get 32 of them right because they actually were fraud okay so
we got 32 of those right but then we didn't get anything else over here like false negative two negatives because there's no ability to have a negative class right
okay let's go the other way right all right so let's assume we set that threshold
all the way up to all the way up to 100 right so now nothing is fraud
everybody becomes rich through arabian princesses and we're all we're all off in good shape did i spell
that right don't think i did anyway it doesn't matter so here the opposite happens right so because we have
zero um we have we have uh no protection at all against fraud
everything gets classified um as not being fraud and so here we have a bunch of true
negatives which that's just good yay all right but we have a bunch of false negatives right so all of these 32 should actually be
over here okay that's the general idea now the thing to keep in mind here and you
know if you want to pause this for a minute and think about it is that there's probably scenarios where
according to what problem you might have right you might adjust that threshold measure right
maybe you have certain tolerances okay for false positives
right so i just want you to stop and think all right we do this sometimes but stop and think about what a scenario
you can think of right that might actually dictate uh the acceptance of false positives
right so just take a second
all right so let's keep going all right we're going to talk about that in class i just keep going i'm not sure like how
long to pause because i want you to pause not me but anyway okay all right so we can further assess our model all right by digging more
into the confusion matrix right so this is where it really starts to live up to its name okay so
we have these true positive rates which is also called sensitivity okay which can also be called recall we
have these false positive rates here which could also be called specificity all right so there's simple calculations
about how we would do this okay so you can just basically plug and chug here all right so you can leave
this true positive true positive false negative i think we walked through this so i'm going to spend a whole lot of time on it
but here you can just you can just see how this works so this is a true positive right so that goes in there right true positive another true
positive right and then false negative so this is basically just a you know like when we got when we
predict true positives how often are we right okay not that good okay all right so
not so good here false positive rate when we predicted false positives how good do we do hey that's not so bad all right we actually want this to be low
all right so we don't want that to be too high all right we want the true positive rate to be high and we want the
false positive rate to be low that part is pretty pretty intuitive i think okay all right
and so staying with this the true positive rate and false positive rate these were generated as a result of a 50
threshold okay right remember what remember how we remember how we did it we adjusted the threshold and then
everything it was up to 100 all of our data was over here right this was 32 and this was 103.
okay you can imagine if we had that scenario right where this was 32 and this was 103
we could actually calculate these scores again okay all right so they
would be probably really good for i don't know really good for false positive rate we're really bad for true positive right right because
we've been predicting anything is true okay but that would mean you notice that this was instead of this
being 100 instead of this being 50 threshold you know then we'd be calculating something at 100 threshold
right so each one of those different percentage points 1 to 100 different
threshold measures they produce true positive and false positive rates okay
so at 30 what is our true positive and false positive rate at 70 threshold what is our true
positive and false positive rate and those just become data points on a axis you just graph them
on the y and x axis and that is how you create a receiver operating curve
okay so each one of these here this you know this is the false true positive rate and false positive rate and what
calculates this all right is just the different thresholds that's it okay and so each one of these
points is just put onto this line for each one of the different thresholds
all the way up and that's how this is calculated okay all right so
all right if we had a very good uh a really good very i mean this is kind of intuitive right
but if we have a really good classifier everything about this is kind of like the general sphere where 50 might occur
this would be way higher like up here okay this is probably isn't too bad or but this is kind of this is a nice
example here you know this is an okay classifier but it's not really learning all that much and then the area under
the curve is just the shaded portion it's like literally the area under the curve all right so
we had a perfect receiver curve and it was like this the area underneath the curve would be 100
okay so it's just the ratio of basically this area that's not out of the curve to this area that's in the
curve and that's it all right so the better classifier higher the roc higher the auc right and
it's much more kind of oriented towards this balance between false positives
and true positives oh here look i did this already okay so you get it so here we go
perfect score this never happens so if this happens you've got big problems so this is this is probably pretty good
you know you're probably you're probably pretty happy you're publishing that paper and hear what this means
all right what this means is that your your your classifier is contributing absolutely nothing right because below
this line here you might as well just be randomly guessing right you got a 50 50 chance to get this right
so you want to see your curve actually show up somewhere so that's not so good all right okay
there's kind of a notional chart you know between 90 and you know 90 and 10 is pretty good
between 80 and 90. you know anything down here is pretty bad so
worthless that's pretty good it's true this is worthless all right
we saw this don't know why it's there again anyway just maybe as a quick reminder okay so there's lots of there's lots of
ways to calculate the accuracy i shouldn't say accuracy it's just not totally fair to evaluate
how good of a model you have all right all right these are all a bunch of a
bunch of different approaches okay we talked about uh we talked about a bunch of these this
is sensitivity all right we talked about that um okay
uh and this is just one minus specificity all right so that's also we talk about
true positive and false positive rate that's what those are you know it's it's it's really confusing here
um so generally these are different ways we've already we've already talked about these you've seen them uh earlier in the slide so you
can just go back and you'll see that the calculations are the same okay an f1 score is an important one that i'm
going to talk about specifically this is well anyway this is accuracy detection rate balance accuracy is good but
f f1 score in particular what it does it's very attenuated towards unbalanced data sets all right so it
uses a harmonic mean all right which is more sensitive okay than just your standard mean and so
it is more sensitive to basically imbalances between these between
precision and recall okay it's particularly useful if you have unbalanced data sets right
if you have a very highly skewed data set say you're like you know 90 10
or something like that or 10 is just your target class okay that's what i mean it's like that maybe 10 is like the the positive class
you're trying to detect that which is often the case in fraud in terms of spam so f1 does a pretty
good job of attenuating just exactly how you're doing with this particular class and
then penalizing you if you're not doing that well so i would i would certainly consider that
especially when you have um especially we have an unbalanced
class that's really what it's designed for okay we also have log loss okay so log loss
measures the uncertainty of the probabilities of your model all right by carrying to the to the true label all right what log loss does
to a certain extent all right before i get into this is it penalizes you for being really
wrong okay so it takes into account how much of the percentage uh miss
accurate percentage inaccuracy of any one label is to the actual true
label all right let me explain it better so like for instance if you happen to predict there's a 10
chance that saying something belongs to the positive class right your model is going to predict that it
belongs to the negative class but if in fact it belongs to the positive class right log loss is going to penalize you
greater than if you happen to predict a label i'd say 45 labeled it as a zero when it should
have been one all right because the basically the the scope of the error for the 10
as compared to 45 or 49 or whatever is much larger so that means that when your model got it wrong it got
it really wrong okay so if you're inside the margin you're guessing it wrong but you're not really
that wrong log loss takes that into account okay all right so if we see a log loss
of one can be expected you know your case when our model gives us less than a forty percent
probability estimate you know selecting the actual class true all right so knowing the base rate is
going to be really important okay so this you can if you get a log loss of one that's how we can translate it right
basically it's like wherever it hits this curve more or less right it gives us about a 40 chance at any one time of being
accurate all right so that's how you that's how you can uh you can use that output of the log loss
number it's also really good i mean it would give you a much better sense uh a holistic idea of just how well your
model is performing okay so instead of just kind of blanketed ones and zeros it will take into account just how wrong the
model is okay and it actually might be quite telling when you use log loss because your model is actually not doing a very
good job at all okay all right so we covered f1 all right and these are extensions of
just like your typical precision recall recovered log loss all right so now we're going to talk about kappa
right so cap is a way you know it's strictly used in um just for outputs of classifiers
all right so it indicates how much better our classifier is performing over the performance of the cat fire then it would be just a guessing you
know that's not super useful but it is really good for multi-class models
all right and it and it was actually used created originally to determine how much
consensus there is among uh expert opinion okay so let's dig a little bit more into it here
all right apparently i don't do that so let me do it here okay i'll give you a reference kappa all
right but basically the way you calculate this is that you know say we had you know and that's good for ensembles but you can do
this from multi-class examples all right if we had three models
okay and they were all trying to predict the true label here right so this is label a label b or these are just inputs c well
let's do it this way x1 x1 x2 x3
i'm not going to do this calculation at hand but basically i'm just going to try it and so we and try and show you how it works just know that x1
all right the true label of x1 all right is 1. okay true label of x2 say is 0
all right and the true label of x3 is one okay so if we had three models
here models x y and z maybe i should change those to abc maybe better
okay all right and every one of these models predicted right that x1 was in
fact a one okay remember it's a one the capital score here would basically be a hundred percent right you understand there's agreement
amongst all three of these all right so almost perfect agreement right so if we assume here we do a
different one so we just do it the opposite right you guys kind of get where i'm going it was zero zero zero oh wait i have to do it the
other way where every model predicted that this was a one all right so there's a hundred
percent consensus that this in fact was a one but we know it to be a zero that means you know this kappa score
would be would be like a zero this would equal like a one all right and then you know the rest of them is
you know kind of generally you can get the idea if there's one one zero here right this would be you know capital score of like you know
66 or something like that all right so it would still be kind of above this threshold we consider kind of a you know a
substantial agreement so we've got two that agree here right that's how campo works it's just
that simple uh there are different approaches to capital but basically it's just oriented around how much agreement there is so
you can do this in the binary okay binary example it would take an aggregate score basically what are the
percentage just add kind of sums all this up and tells you uh over uh
your entire um you know prediction to label how accurate you are it really
works well especially tuned i mean it's basically just i mean that it in binary examples it's it's
basically just the detection rate but in multi-class samples it works really well right so you can scale it
across if you have um you know three labels that you're trying to to to predict how well
across those three levels is there agreement you know in terms of the way that things are working with the um
with the actual predicted class okay we'll show examples of all these in the code i don't know why i go through and
erase all this probably easier for me just to keep it there but anyway okay so that's generally how kappa works it
just shows you kind of this uh percentage of agreement across the different labels all right
all right let's keep going i don't know about another definition but basically what i'm talking about here is that
we're just moving into regression okay another name for regression is functional approximation
all right so here what we're trying to do um is our target variable is our dependent variable other variables are independent
i think that's probably a you know kind of a language that people are familiar with
i'm going to go in here all right so let's just keep going so let's just talk about how we would predict basically instead of doing
classification here we're just talking about continuous all right so instead of categorical variable now we're moving into a continuous one all right
so there's different metrics that you would use to assess a continuous variable right kind of in a regression format these are
the standard ones that i think a lot of people are familiar with mean squared error right that's just a difference between the predicted and
actual values okay between the predicted and actual values you just sum it up all right
all right this is the same as above except you take the square root all right
to put the error back in terms of the dependent variable all right so here what you're doing is you're taking the average you're squaring it and then
you're summing it and here what you're doing is you're taking out the square because you're just taking the square root all right so you put it back
in the language of the dependent variable so that gives you a better idea of exactly how far off
you are okay this is very similar all right except uh taking the square you just use the
average you think the absolute value instead of squaring all right these are what these calculations look like here you're just taking the absolute value here you're
squaring the differences all right and then root mean squared error you'll see that this calculation
this equation here is exactly the same as this one right it just takes the square root and
then divides it by the total number all right actually it's alright that's part of it basically the
only thing it's doing is adding the square root so this one here is i want you to we're
not we don't do a whole lot of continuous variable prediction in this class we probably should do more but what this
one does all right is it just normalizes all right that root mean squared error
by dividing it by the maximum and minimum of your dependent variable
okay so basically what it does like remember up here right we take the mean squared error all
right but this is not going to be in the language of the dependent root mean squared error will be all right it puts it back inside
the basically they're basically in the same values as uh as our dependent what this does
is it puts it in context uh to the range of our dependent variable okay so we
have a really big range we have a small root mean squared error awesome okay
a really big root mean squared error but a very small range not so good okay so that means here what
you can literally calculate is the you can think of this almost like kind of like confidence intervals to a certain extent all right where say like
what percentage of our root mean squared error is actually uh you know what percentage of the range
of our dependent variable is accounted for inside the root mean squared error we want that to be small
okay so if for instance i don't know if we have a range of our dependent variable is between 0 and 100 and our root mean
squared error say is 50 i mean that at any one point it could be 50 off inside that range that's not
so great if we have a range between zero and a million right and our mean squared error is only 50 that's
that's not quite as bad right so it helps put that into context that's what this does here it's all right it just normalizes that inside the
range all right last couple of things here i just want to throw these at you because
we're going to start talking about them as we start to get into more complicated machine learning algorithms
all right linear regression as you may be aware has a overfitting problem
all right has a tendency to learn as many variables as you give it right go down to the very lowest level
what we call this is overfitting it basically has a very low bias right so it just memorizes the data okay
i bring this up because it's associated with thresholding because if you want to adjust the thresholding it will help for overfitting
all right okay but it also it also kind of um stems into what we're
going to be talking about kind of moving forward so i'm trying to introduce these topics a bit a bit earlier uh
so here what can be said is regression and sparsity so what we want when we
think about and this this is sparsely this is true of all you know kind of all models
all right this should be our goal there's a whole field of study about this but we want our models to be as accurate
as possible with as many features as possible that's basically sparsity all right it allows them to be agile
all right that is the data grows all right it's not heavily dependent on a large number of covariates all right that we
have a bunch that are really predictive and we use those and we power those up as much as possible it's computationally
more efficient to think of that as as well so sparsity in terms of our models helps us not only in generalizability
but also in computational resources when they're deployed out in the real world all right so sparsity is a real thing
all right so let's talk about it so what do we mean by sparse okay so we have three examples here and why
does it relate to variance and bias we have three examples one two you guys can count right three okay
all right so here what we have is like your typical regression model right if unattuned in the right way it memorizes these features to such a
way where there is no generalizability right the pattern with which it is put on to
this data set exactly matches the pattern of the data okay so this is called high variance
okay so it's over fit all right low bias high variance all right so that's kind of what we
would see over here right all right this one here is going to have high bias
okay it's going to be underfit and these are generalizations sorry these are generalizations all right but we have much
fewer features here all right so there's patterns inside the data which we're just not accounting for okay so this one
is under fit so here we have high bias okay and low variance
so that's what we see down here all right so we have this tight little cluster
okay and here we have kind of a pretty high pattern associated with where the data points
are all right and then here this is our you know the porridge is just right kind of thing all right we
see that the line isn't exactly memorizing the dots right and we don't see it go like this
right we see that it's generalizable because it fits near them all right and also it uses
fewer features okay so that's what we mean by it's fit kind of just right
all right so the way the where that would sit would probably be kind of in the middle here okay
all right where the grouping would be tighter than this one right but it would be on target
all right so it'd probably be a combination of these things here all right so this is low bias low variance just to work through these
right where you see it's just you know super tight here in the middle and this is high bias
and high variance all right so it's off the target and scattered okay so this is kind of like a classic
example using this target thing you know you'll see these all over the place all right but really what this lacks is kind
of this fifth component here where we see that the data probably you know unlike this portion of
it right it probably goes out to this second rim all right but it's not that far off
something like that but it's on target okay and that's what we mean that it fits just right right maybe it doesn't leave
this second white circle right but it is on target okay so we've introduced a bunch of
concept here all right to include kind of to include how to manage thresholds we've talked about different performance
metrics including f1 and kappa and log loss those are all kind of extensions of our typical precision
recall trade-off we also started talking here about this bias variance trade-off right
now the thing is to put these two things together all right i want you to think about what you what
you could do all right with thresholding as it relates to to the bias variance trade-off
okay that's these relate right because you can think about how these dots represent
right in general kind of performance of the model okay we'll talk more about this next week but
thresholding can be used to adjust exactly what this looks like all right
in general though you'll talk about kind of the trade-off in in machine learning models as it relates
to variance bias trade-off all right it's a huge component of what we think about when we think about uh the applications machine
learning methods and models in general right and how close they can grow so it relates to some of the topics that we've been
discussing here all right but it also kind of is an extension all right of that all right so i just
wanted to walk you all through that and uh that's it for this week we'll talk more about
all this next week and we'll have some code examples um to go through as well to just to kind of
reinforce these like we always do so i will see you then